{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN/KYeH0gqE6rtYdyeR75Lj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"rFHJyMrr0u4L"},"outputs":[],"source":["pip install -q kaggle"]},{"cell_type":"code","source":["!mkdir ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json"],"metadata":{"id":"CT1neyLO01et"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!kaggle datasets download thomasdubail/brain-tumors-256x256"],"metadata":{"id":"Zqg2OvqQ01hw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! unzip brain-tumors-256x256.zip"],"metadata":{"id":"CHhxYBtAk9H8","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import shutil\n","import random\n","\n","# Set seed for reproducibility\n","random.seed(42)\n","\n","# Paths\n","source_dir = '/content/Data'\n","base_dir = '/content/output'  # Output folder\n","train_dir = os.path.join(base_dir, 'train')\n","test_dir = os.path.join(base_dir, 'test')\n","\n","# Create output directories\n","for split_dir in [train_dir, test_dir]:\n","    for category in ['glioma_tumor', 'meningioma_tumor', 'normal', 'pituitary_tumor']:\n","        os.makedirs(os.path.join(split_dir, category), exist_ok=True)\n","\n","# Split function\n","def split_data(source, train_dest, test_dest, split_ratio=0.8):\n","    files = [f for f in os.listdir(source) if f.endswith('.jpg')]\n","    random.shuffle(files)\n","\n","    split_point = int(len(files) * split_ratio)\n","    train_files = files[:split_point]\n","    test_files = files[split_point:]\n","\n","    for file_list, dest_dir in [(train_files, train_dest), (test_files, test_dest)]:\n","        for fname in file_list:\n","            src_path = os.path.join(source, fname)\n","            dst_path = os.path.join(dest_dir, fname)\n","            try:\n","                shutil.copyfile(src_path, dst_path)\n","            except:\n","                pass  # Ignore corrupted images\n","\n","# Split cats and dogs\n","split_data(os.path.join(source_dir, 'glioma_tumor'),\n","           os.path.join(train_dir, 'glioma_tumor'),\n","           os.path.join(test_dir, 'glioma_tumor'))\n","\n","split_data(os.path.join(source_dir, 'meningioma_tumor'),\n","           os.path.join(train_dir, 'meningioma_tumor'),\n","           os.path.join(test_dir, 'meningioma_tumor'))\n","\n","split_data(os.path.join(source_dir, 'normal'),\n","           os.path.join(train_dir, 'normal'),\n","           os.path.join(test_dir, 'normal'))\n","\n","split_data(os.path.join(source_dir, 'pituitary_tumor'),\n","           os.path.join(train_dir, 'pituitary_tumor'),\n","           os.path.join(test_dir, 'pituitary_tumor'))"],"metadata":{"id":"WmgyVtbwk9Ez"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","# Count number of images in each set\n","for split in ['train', 'test']:\n","    print(f\"\\n{split.upper()} SET:\")\n","    for category in ['glioma_tumor', 'meningioma_tumor', 'normal', 'pituitary_tumor']:\n","        folder_path = f\"/content/output/{split}/{category}\"\n","        count = len(os.listdir(folder_path))\n","        print(f\"  {category}: {count} images\")\n"],"metadata":{"id":"v-XDPpXhk9B2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","train_datagen = ImageDataGenerator(\n","    rescale=1./255,\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")\n","\n","test_datagen = ImageDataGenerator(rescale=1./255)"],"metadata":{"id":"-UVGLkdak85l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers, models, regularizers\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","\n","model = models.Sequential([\n","    layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001), input_shape=(256, 256, 3)),\n","    layers.MaxPooling2D(2, 2),\n","    layers.Dropout(0.25),\n","\n","    layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n","    layers.MaxPooling2D(2, 2),\n","    layers.Dropout(0.25),\n","\n","    layers.Conv2D(128, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n","    layers.MaxPooling2D(2, 2),\n","    layers.Dropout(0.25),\n","\n","    layers.Flatten(),\n","    layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n","    layers.Dropout(0.5),\n","    layers.Dense(4, activation='softmax')  # For 4 classes\n","])\n","\n","# Compile\n","model.compile(optimizer='adam',\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","model.summary()"],"metadata":{"collapsed":true,"id":"rloajlWHLa3i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import tensorflow as tf\n","# from tensorflow.keras import layers, models\n","\n","# model = models.Sequential([\n","#     layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)),\n","#     layers.MaxPooling2D(2, 2),\n","#         Dropout(0.25),\n","\n","\n","#     layers.Conv2D(64, (3, 3), activation='relu'),\n","#     layers.MaxPooling2D(2, 2),\n","#         Dropout(0.25),\n","\n","\n","#     layers.Conv2D(128, (3, 3), activation='relu'),\n","#     layers.MaxPooling2D(2, 2),\n","#         Dropout(0.25),\n","\n","\n","#     layers.Flatten(),\n","#     layers.Dense(512, activation='relu'),\n","#         Dropout(0.5),\n","#     layers.Dense(4, activation='softmax')  # Change to softmax if more than two classes\n","# ])\n","\n","# model.compile(optimizer='adam',\n","#               loss='categorical_crossentropy',  # Use 'categorical_crossentropy' for multi-class\n","#               metrics=['accuracy'])\n","\n","# model.summary()"],"metadata":{"id":"A8QDpJa-oDNK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_generator = train_datagen.flow_from_directory(\n","    '/content/output/train',\n","    target_size=(256, 256),\n","    batch_size=32,\n","    class_mode='categorical'  # or 'categorical' for multi-class classification\n",")\n","\n","validation_generator = test_datagen.flow_from_directory(\n","    '/content/output/test',\n","    target_size=(256, 256),\n","    batch_size=32,\n","    class_mode='categorical'\n",")\n"],"metadata":{"id":"UZ7x7qCAoXYD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import math # import math module\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","\n","# Callbacks\n","early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n","\n","history = model.fit(\n","    train_generator,\n","    steps_per_epoch=math.ceil(train_generator.samples / train_generator.batch_size), # use batch_size from train_generator\n","    epochs=20,\n","    validation_data=validation_generator,\n","    validation_steps=math.ceil(validation_generator.samples / validation_generator.batch_size), # use batch_size from validation_generator\n","    callbacks=[early_stop, reduce_lr]\n",")\n","\n"],"metadata":{"id":"wMtM6TM-MuqD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"25790e74-0bba-4081-e48c-f59532119024"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 533ms/step - accuracy: 0.2714 - loss: 4.4072 - val_accuracy: 0.2319 - val_loss: 1.9582 - learning_rate: 0.0010\n","Epoch 2/20\n","\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 477ms/step - accuracy: 0.3101 - loss: 1.8371 - val_accuracy: 0.3478 - val_loss: 1.5991 - learning_rate: 0.0010\n","Epoch 3/20\n","\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 477ms/step - accuracy: 0.2800 - loss: 1.5849 - val_accuracy: 0.2915 - val_loss: 1.5093 - learning_rate: 0.0010\n","Epoch 4/20\n","\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 479ms/step - accuracy: 0.3124 - loss: 1.4840 - val_accuracy: 0.2963 - val_loss: 1.5002 - learning_rate: 0.0010\n","Epoch 5/20\n","\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 476ms/step - accuracy: 0.3514 - loss: 1.4226 - val_accuracy: 0.3092 - val_loss: 1.4899 - learning_rate: 0.0010\n","Epoch 6/20\n","\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 477ms/step - accuracy: 0.4009 - loss: 1.3493 - val_accuracy: 0.3237 - val_loss: 1.4797 - learning_rate: 0.0010\n","Epoch 7/20\n","\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 477ms/step - accuracy: 0.4283 - loss: 1.3228 - val_accuracy: 0.4654 - val_loss: 1.3366 - learning_rate: 0.0010\n","Epoch 8/20\n","\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 473ms/step - accuracy: 0.4347 - loss: 1.2805 - val_accuracy: 0.4348 - val_loss: 1.3740 - learning_rate: 0.0010\n","Epoch 9/20\n","\u001b[1m66/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m5s\u001b[0m 459ms/step - accuracy: 0.4459 - loss: 1.2884"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","epochs = range(1, len(acc) + 1)\n","\n","plt.figure(figsize=(12, 5))\n","\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs, acc, 'bo-', label='Training accuracy')\n","plt.plot(epochs, val_acc, 'ro-', label='Validation accuracy')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs, loss, 'bo-', label='Training loss')\n","plt.plot(epochs, val_loss, 'ro-', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n","plt.show()\n"],"metadata":{"id":"E8g8-Ddturmb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_loss, test_acc = model.evaluate(validation_generator, steps=50)\n","print('Test accuracy:', test_acc)\n"],"metadata":{"id":"BmkIblv5uvPx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing import image\n","import numpy as np\n","\n","# Load and preprocess the image\n","img = image.load_img('/content/Data/pituitary_tumor/P_107.jpg', target_size=(256, 256))\n","img_array = image.img_to_array(img)\n","img_array = np.expand_dims(img_array, axis=0)  # Create batch dimension\n","img_array /= 255.0  # Normalize pixel values\n","\n","# Make prediction\n","prediction = model.predict(img_array)\n","\n","# Define the class names corresponding to the model output indices\n","# Adjust the list to reflect the actual class order used during training\n","class_names = ['glioma_tumor', 'meningioma_tumor', 'normal', 'pituitary_tumor']\n","\n","# Get the index of the highest probability\n","predicted_index = np.argmax(prediction)\n","predicted_class = class_names[predicted_index]\n","\n","print(\"Prediction:\", predicted_class)\n"],"metadata":{"id":"85Ph3vT8yRmc"},"execution_count":null,"outputs":[]}]}